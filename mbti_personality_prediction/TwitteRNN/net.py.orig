import re
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
from nltk.tokenize import word_tokenize
from multiprocessing import Pool
from tqdm import tqdm

from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
from keras.layers.convolutional import Convolution1D, MaxPooling1D
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.model_selection import train_test_split


class TwitteRNN:

    def __init__(self, tweets, sents, vocab_size, unk_label=0, n_cores=8):
        self.data = None
        self.tweets = tweets
        self.cleaned_tweets = None
        self.tokenized_tweets = None
        self.words = None
        self.X = None
        self.y = np.asarray(sents)
        self.max_seq_len = None
        self.n_examples = None
        self.X_train = None
        self.y_train = None
        self.X_val = None
        self.y_val = None

        self.vocab_size = vocab_size
        self.unk_label = unk_label
        self.dictionary = None
        self.reverse_dictionary = None
        self.unk_count = None

        self.model = None

        self.n_cores = n_cores

        self.functions = [
            self.clean_tweets,
            self.tokenize_tweets,
            self.get_words,
            self.build_mapping,
            self.map_all
        ]

    def clean_tweets(self):
        # Take a list of tweets and output cleaned tweets list

        cleaned_tweets = []

        for tweet in self.tweets:
            tweet_temp = " ".join(tweet.split())  # strip duplicate whitespace
            tweet_temp = re.sub("\.{4,}", "...", tweet_temp)  # replace multiple ..... with ...
            tweet_temp = re.sub(r"^https?://.*[\r\n]*", "", tweet_temp)  # strip URLs
            tweet_temp = re.sub(r"[@#$%^&*~]", "", tweet_temp)  # remove some weird characters
            tweet_temp = tweet_temp.lower()  # to lowercase
            """ replace 3+ consecutive occurrences of a char by "n" chars to preserve 'excitement' (watch out with that!)
            since it might change the vocabulary for words such as 'too' etc."""
            matches = re.findall(r"((\w)\2{2,})", tweet_temp)
            n = 2
            if matches is not None:
                for match in matches:
                    tweet_temp = re.sub(match[0], match[1] * n, tweet_temp)
            cleaned_tweets.append(tweet_temp)

        self.cleaned_tweets = cleaned_tweets

        return self.cleaned_tweets

    def tokenize_tweets(self, chunk_size=10000):
        # Tokenize tweets using multiple CPU cores if needed

        tokenized_tweets = []

        bar = tqdm(total=len(self.cleaned_tweets))
        pool = Pool(processes=self.n_cores, maxtasksperchild=1)
        for text in pool.imap(word_tokenize, self.cleaned_tweets, chunksize=chunk_size):
            tokenized_tweets.append(text)
            bar.update()

        self.tokenized_tweets = tokenized_tweets

        return self.tokenized_tweets

    def get_words(self):
        # Concatenate all tweets into one huge list of words (needed for word freq counts)

        all_words = []
        for tweet in self.tokenized_tweets:
            for word in tweet:
                all_words.append(word)

        self.words = all_words

        return self.words

    def build_mapping(self):
        # Build dictionary of (word -> order of frequency) mapping

        count = [["UNK", -1]]
        count.extend(Counter(self.words).most_common(self.vocab_size - 1))
        dictionary = dict()
        for word, _ in count:
            dictionary[word] = len(dictionary)
        reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))

        self.dictionary = dictionary
        self.reverse_dictionary = reverse_dictionary

        return self.dictionary

    def map_all(self):
        # Transform tweets to format accepted by LSTM

        X = []
        X_lengths = []
        for tweet in self.tokenized_tweets:

            mapping = []
            for word in tweet:
                if word in self.dictionary:
                    idx = self.dictionary[word]
                else:
                    idx = self.unk_label

                mapping.append(idx)
            X.append(mapping)
            X_lengths.append(len(mapping))

        X = np.asarray(X)
        self.X = X
        self.max_seq_len = max(X_lengths)
        self.n_examples = X.shape[0]

        return self.X

    def preprocess(self):
        for function in self.functions:
            print("Running: " + str(function))
            self.data = function()

    def train(self, embedded_vec_len=16, filters=32, kernel_size=3, padding="same", activation_conv="relu",
<<<<<<< HEAD
              pool_size=2, test_size=0.2, seed=42, train_n=30000, epochs=3, batch_size=256, dropout=0.5):
=======
              pool_size=2, test_size=0.2, seed=42, train_n=30000, epochs=3, batch_size=256, dropout=0.5,
              early_stop_delta=0.01, early_stop_patience=3,
              weights_file=None):
>>>>>>> 9f4f9ebe8bccdbfd3ceef20c41301ba75be47c38

        # Split data intro training and validation
        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(self.X, self.y,
                                                                              test_size=test_size, random_state=seed)

        # Truncate and pad input sequences
        self.X_train = sequence.pad_sequences(self.X_train, maxlen=self.max_seq_len)
        self.X_val = sequence.pad_sequences(self.X_val, maxlen=self.max_seq_len)

        filepath = "weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5"
        checkpointer = ModelCheckpoint(filepath="./model_checkpoints/" + filepath,
                                       monitor="val_acc", verbose=1, save_best_only=True, mode="max")
        early_stopper = EarlyStopping(monitor="val_acc", min_delta=early_stop_delta,
                                      patience=early_stop_patience, verbose=1)
        callbacks_list = [checkpointer, early_stopper]

        # Define and train the model, yay! :D
        model = Sequential()
        model.add(Embedding(self.vocab_size, embedded_vec_len, input_length=self.max_seq_len))
        model.add(Convolution1D(filters=filters, kernel_size=kernel_size, padding=padding, activation=activation_conv))
        model.add(MaxPooling1D(pool_size=pool_size))
        model.add(LSTM(100))
        model.add(Dropout(dropout))
        model.add(Dense(1, activation="sigmoid"))
        model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
        print(model.summary())

        if weights_file is not None:
            model.load_weights(weights_file)
        else:
            history = model.fit(self.X_train[:train_n], self.y_train[:train_n], epochs=epochs, batch_size=batch_size,
                                validation_split=0.2, callbacks=callbacks_list)

        self.model = model

        plt.plot(history.history["val_acc"])

    def predict(self, tweet):
        # Preprocess the tweet
        tweet_temp = " ".join(tweet.split())  # strip duplicate whitespace
        tweet_temp = re.sub("\.{4,}", "...", tweet_temp)  # replace multiple ..... with ...
        tweet_temp = re.sub(r"^https?://.*[\r\n]*", "", tweet_temp)  # strip URLs
        tweet_temp = re.sub(r"[@#$%^&*~]", "", tweet_temp)  # remove some weird characters
        tweet_temp = tweet_temp.lower()  # to lowercase
        """ replace 3+ consecutive occurrences of a char by "n" chars to preserve 'excitement' (watch out with that!)
        since it might change the vocabulary for words such as 'too' etc."""
        matches = re.findall(r"((\w)\2{2,})", tweet_temp)
        n = 2
        if matches is not None:
            for match in matches:
                tweet_temp = re.sub(match[0], match[1] * n, tweet_temp)

        cleaned_tweet = tweet_temp
        tokenized_tweet = word_tokenize(cleaned_tweet)

        mapping = []
        unk_count = 0
        for word in tokenized_tweet:
            if word in self.dictionary:
                idx = self.dictionary[word]
            else:
                idx = self.unk_label
                unk_count += 1
            mapping.append(idx)
            self.unk_count = unk_count
        mapping = np.asarray(mapping)

        # Truncate and pad input sequence
        padded_mapping = sequence.pad_sequences([mapping], maxlen=self.max_seq_len)

        pred = self.model.predict(padded_mapping)

        return pred

    def evaluate(self, X_test, y_test):
        model_eval = self.model.evaluate(X_test, y_test)
        print("Accuracy: %.2f%%" % (model_eval[1]*100))
